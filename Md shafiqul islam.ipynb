{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data_folder = './data/'\n",
    "sea_state = 3 # Can be 1, 2 or 3\n",
    "realization = 7 # Can be 1, 2, ... 8\n",
    "\n",
    "prediction_time=5 #in s.\n",
    "\n",
    "filename = 'Ship_{:d}_{:d}.csv'.format(sea_state,realization)\n",
    "input_data = pd.read_csv(data_folder+filename)\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(input_data)\n",
    "\n",
    "# plt.plot(df['Rot_Z'][0:5000])\n",
    "# plt.plot(df['Z'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT of the rotation pattern.\n",
    "\n",
    "from scipy.fft import fft, ifft\n",
    "import numpy as np\n",
    "\n",
    "# Number of sample points\n",
    "\n",
    "time=df['Time']\n",
    "\n",
    "X=df['X'].to_numpy()\n",
    "Y=df['Y'].to_numpy()\n",
    "Z=df['Z'].to_numpy()\n",
    "\n",
    "\n",
    "Rot_X= df['Rot_X'].values.tolist()\n",
    "Rot_Y= df['Rot_Y'].values.tolist()\n",
    "Rot_Z= df['Rot_Z'].values.tolist()\n",
    "\n",
    "time_train=time[0:19980]\n",
    "time_test=time[19980:24000]\n",
    "\n",
    "\n",
    "X_train=X[0:19980]\n",
    "X_test=X[19980:-1]\n",
    "\n",
    "Y_train=Y[0:19980]\n",
    "Y_test=Y[19980:-1]\n",
    "\n",
    "Z_train=Z[0:19980]\n",
    "Z_test=Z[19980:-1]\n",
    "\n",
    "Rot_X_train=Rot_X[0:19980]\n",
    "Rot_X_test=Rot_X[19980:24000]\n",
    "\n",
    "Rot_Y_train=Rot_Y[0:19980]\n",
    "Rot_Y_test=Rot_Y[19980:24000]\n",
    "\n",
    "Rot_Z_train=Rot_Z[0:19980]\n",
    "Rot_Z_test=Rot_Z[19980:24000]\n",
    "\n",
    "max_X_train=max(X_train)\n",
    "max_Y_train=max(Y_train)\n",
    "max_Z_train=max(Z_train)\n",
    "\n",
    "\n",
    "\n",
    "max_Rot_X_train=max(Rot_X_train)\n",
    "max_Rot_Y_train=max(Rot_Y_train)\n",
    "max_Rot_Z_train=max(Rot_Z_train)\n",
    "\n",
    "# Rot_Y=df['Rot_Y'].to_numpy()\n",
    "\n",
    "xf = np.linspace(0.0, 19980/20, 19980//60)\n",
    "\n",
    "fft_Rot_X_train = fft(Rot_X_train)\n",
    "fft_Rot_Y_train = fft(Rot_Y_train)\n",
    "fft_Rot_Z_train = fft(Rot_Z_train)\n",
    "\n",
    "# fft_Rot_Y_short = np.linspace(0.0, 20, 400)\n",
    "# fft_Rot_X_short = np.linspace(0.0, 20, 400)\n",
    "fft_Rot_X_short=(2/19980*fft_Rot_X_train[0:400])\n",
    "fft_Rot_Y_short=(2/19980*fft_Rot_Y_train[0:400])\n",
    "fft_Rot_Z_short=(2/19980*fft_Rot_Z_train[0:400])\n",
    "\n",
    "# max_fft_Rot_Y_short=max(fft_Rot_Y_short)\n",
    "# max_fft_Rot_Y_short=max(fft_Rot_Y_short)\n",
    "# max_fft_Rot_Y_short=max(fft_Rot_Y_short)\n",
    "\n",
    "x_short=xf\n",
    "# plt.plot(fft_Rot_Y_short,fft_Rot_Y_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss 5.866237640380859\n",
      "Epoch: 100 loss 5.852173805236816\n",
      "Epoch: 200 loss 5.8387579917907715\n",
      "Epoch: 300 loss 5.825872898101807\n",
      "Epoch: 400 loss 5.813634872436523\n",
      "Epoch: 500 loss 5.802046298980713\n",
      "Epoch: 600 loss 5.791093349456787\n",
      "Epoch: 700 loss 5.780697345733643\n",
      "Epoch: 800 loss 5.77076530456543\n",
      "Epoch: 900 loss 5.761338710784912\n",
      "w1:  tensor([[ 0.4680, -0.2787,  0.7074, -0.0204,  0.2259,  0.8570],\n",
      "        [-0.5436, -0.4636, -0.0060, -0.4232, -0.6786,  0.0383],\n",
      "        [ 0.5548,  0.2469,  0.4091,  0.3421,  0.3249, -0.5792],\n",
      "        [ 0.4536,  0.1001,  0.8486, -0.4985,  0.1340, -0.8540],\n",
      "        [ 0.3555, -0.2765,  0.1088,  1.1368, -0.1808,  0.7309],\n",
      "        [ 0.0170,  0.6897, -0.2726, -0.7028, -0.4701,  0.2341],\n",
      "        [-0.0544, -0.2094,  0.6015, -0.4474,  0.5277, -0.0689]],\n",
      "       requires_grad=True)\n",
      "w2:  tensor([[ 0.4105],\n",
      "        [ 0.1549],\n",
      "        [ 0.9199],\n",
      "        [ 1.0481],\n",
      "        [ 0.2456],\n",
      "        [-0.9126]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Frequency prediction\n",
    "#https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/\n",
    "#success with Rot_X\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import torch.nn.init as init\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "input_size, hidden_size, output_size = 7, 6, 1\n",
    "epochs = 1000\n",
    "# seq_length = 20\n",
    "lr = 0.000001 #0.00005\n",
    "\n",
    "# data_time_steps = np.linspace(2, 10, seq_length + 1)\n",
    "# data = np.sin(data_time_steps)\n",
    "\n",
    "x_=np.divide(fft_Rot_X_short,max(fft_Rot_X_short))\n",
    "max_fft_Rot_X_short=max(fft_Rot_X_short)\n",
    "\n",
    "data_Rot_X=x_.copy(order='C')\n",
    "seq_length=len(data_Rot_X)\n",
    "\n",
    "data_Rot_X.resize((seq_length + 1, 1))\n",
    "\n",
    "x = Variable(torch.Tensor(data_Rot_X[:-1]).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.Tensor(data_Rot_X[1:]).type(dtype), requires_grad=False)\n",
    "\n",
    "w1_Rot_X = torch.FloatTensor(input_size, hidden_size).type(dtype)\n",
    "# init.normal(w1_Rot_X, 0, 0.3)\n",
    "w1_Rot_X=torch.tensor([[ 0.4686, -0.2785,  0.7082, -0.0189,  0.2265,  0.8570],\n",
    "        [-0.5439, -0.4637, -0.0066, -0.4237, -0.6787,  0.0386],\n",
    "        [ 0.5543,  0.2468,  0.4083,  0.3409,  0.3246, -0.5786],\n",
    "        [ 0.4538,  0.1002,  0.8488, -0.4980,  0.1340, -0.8546],\n",
    "        [ 0.3559, -0.2763,  0.1110,  1.1380, -0.1804,  0.7301],\n",
    "        [ 0.0165,  0.6896, -0.2735, -0.7036, -0.4704,  0.2348],\n",
    "        [-0.0529, -0.2090,  0.6033, -0.4442,  0.5282, -0.0718]],\n",
    "       requires_grad=True)\n",
    "w1_Rot_X =  Variable(w1_Rot_X, requires_grad=True)\n",
    "\n",
    "w2_Rot_X = torch.FloatTensor(hidden_size, output_size).type(dtype)\n",
    "# init.normal(w2_Rot_X, 0, 0.4)\n",
    "\n",
    "w2_Rot_X=torch.tensor([[ 0.4116],\n",
    "        [ 0.1517],\n",
    "        [ 0.9228],\n",
    "        [ 1.0486],\n",
    "        [ 0.2479],\n",
    "        [-0.9111]], requires_grad=True)\n",
    "w2_Rot_X = Variable(w2_Rot_X, requires_grad=True)\n",
    "\n",
    "def forward(input, context_state, w1_Rot_X, w2_Rot_X):\n",
    "  xh = torch.cat((input, context_state), 1)\n",
    "  context_state = torch.tanh(xh.mm(w1_Rot_X))\n",
    "  out = context_state.mm(w2_Rot_X)\n",
    "  return  (out, context_state)\n",
    "\n",
    "for i in range(epochs):\n",
    "  total_loss = 0\n",
    "  context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=True)\n",
    "\n",
    "  for j in range(x.size(0)):\n",
    "    input  = x[j:(j+1)]\n",
    "    target = y[j:(j+1)]\n",
    "    (pred, context_state) = forward(input, context_state, w1_Rot_X, w2_Rot_X)\n",
    "    loss = (pred - target).pow(2).sum()/2\n",
    "    total_loss += loss\n",
    "    loss.backward()\n",
    "    w1_Rot_X.data -= lr * w1_Rot_X.grad.data\n",
    "    w2_Rot_X.data -= lr * w2_Rot_X.grad.data\n",
    "    w1_Rot_X.grad.data.zero_()\n",
    "    w2_Rot_X.grad.data.zero_()\n",
    "    context_state = Variable(context_state.data)\n",
    "  if i % 100 == 0:\n",
    "     print(\"Epoch: {} loss {}\".format(i, total_loss.data))\n",
    "\n",
    "context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=False)\n",
    "predictions_Rot_X_short = []\n",
    "\n",
    "print(\"w1: \",w1_Rot_X)\n",
    "print(\"w2: \",w2_Rot_X)\n",
    "\n",
    "for i in range(x.size(0)):\n",
    "  input = x[i:i+1]\n",
    "  (pred, context_state) = forward(input, context_state, w1_Rot_X, w2_Rot_X)\n",
    "  context_state = context_state\n",
    "  predictions_Rot_X_short.append(pred.data.numpy().ravel()[0])\n",
    "\n",
    "# plt.plot(y)\n",
    "# plt.plot(predictions_Rot_X_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.25580844-0.11391808j 0.26117398-0.10259012j 0.26712823-0.09135543j ...\n",
      " 0.24261765-0.14986876j 0.24661173-0.13742619j 0.25097849-0.12548455j]\n"
     ]
    }
   ],
   "source": [
    "#generate predicted Rot_X wave\n",
    "Rot_X_predicted=ifft(predictions_Rot_X_short[0:-1] + [0]*19580+[0]*prediction_time*20)\n",
    "temp_fft_Rot_X_train_reverse=np.divide( Rot_X_predicted,max(Rot_X_predicted))\n",
    "Rot_X_predicted=np.multiply(temp_fft_Rot_X_train_reverse, max_Rot_X_train)\n",
    "print((Rot_X_predicted))\n",
    "# print(len(Rot_Y_predicted))\n",
    "# plt.plot((Rot_X_predicted),c='r')\n",
    "# plt.plot(Rot_Y_train)\n",
    "# plt.plot(np.multiply(temp_y_fft_Rot_Y_train_reverse,max_y_fft_Rot_Y_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared_train_Rot_X:  10.34676767566113\n"
     ]
    }
   ],
   "source": [
    "#Rot_X error\n",
    "actual_data_Rot_X=Rot_X[0:len(Rot_X_predicted)][-20*prediction_time:]\n",
    "predicted_data_Rot_X=Rot_X_predicted[-20*prediction_time:]\n",
    "#error for x\n",
    "ss_res_train=[]\n",
    "for k in range(len(predicted_data_Rot_X)):\n",
    "    ss_res_train.append((actual_data_Rot_X[k]- predicted_data_Rot_X[k])**2)\n",
    "ss_tot_train=[]\n",
    "for k in range(len(predicted_data_Rot_X)):\n",
    "    ss_tot_train.append((actual_data_Rot_X[k]-np.mean(actual_data_Rot_X))**2)\n",
    "r_squared_train_Rot_X = np.abs(1 - (np.sum( ss_res_train)/np.sum(ss_tot_train)))\n",
    "\n",
    "# plt.plot(predicted_data_Rot_X,actual_data_Rot_X)\n",
    "print(\"r_squared_train_Rot_X: \",np.abs(np.sum(ss_res_train))/len(predicted_data_Rot_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-b0aed8f18188>:37: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(w1_y, 0, 0.3)\n",
      "<ipython-input-6-b0aed8f18188>:46: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(w2_y, 0, 0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss 2607.40087890625\n",
      "Epoch: 20 loss 2280.24267578125\n",
      "Epoch: 40 loss 2271.918701171875\n",
      "Epoch: 60 loss 2271.382568359375\n",
      "Epoch: 80 loss 2271.184326171875\n",
      "w1:  tensor([[ 0.6012,  0.6166,  0.2862,  0.1950],\n",
      "        [ 0.0923,  0.0244,  0.1502,  0.5072],\n",
      "        [ 0.0235,  0.3742,  0.1996, -0.6959],\n",
      "        [-0.1187,  0.5691,  0.3316, -0.1720],\n",
      "        [ 0.1494,  0.0198, -0.1890,  0.6726]], requires_grad=True)\n",
      "w2:  tensor([[ 0.6033],\n",
      "        [-0.0581],\n",
      "        [ 0.4840],\n",
      "        [-0.0777]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/\n",
    "#success with Rot_X vs y\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "input_size, hidden_size, output_size = 5, 4, 1\n",
    "epochs = 100\n",
    "# seq_length = 20\n",
    "lr = 0.00006 #0.00005\n",
    "\n",
    "# data_time_steps = np.linspace(2, 10, seq_length + 1)\n",
    "# data = np.sin(data_time_steps)\n",
    "\n",
    "Rot_X_=np.divide(Rot_X_train,max(Rot_X_train))\n",
    "# print(y_fft_short)\n",
    "data_Rot_X=Rot_X_.copy(order='C')\n",
    "seq_length=len(data_Rot_X)\n",
    "data_Rot_X.resize((seq_length + 1, 1))\n",
    "X_train_max=max(X_train)\n",
    "\n",
    "Y_=np.divide(Y_train,max(Y_train))\n",
    "# print(y_fft_short)\n",
    "data_Y=Y_.copy(order='C')\n",
    "seq_length=len(data_Y)\n",
    "data_Y.resize((seq_length + 1, 1))\n",
    "\n",
    "# print(np.shape(data),data)\n",
    "\n",
    "y = Variable(torch.Tensor(data_Y[1:]).type(dtype), requires_grad=False)\n",
    "rot_x = Variable(torch.Tensor(data_Rot_X[1:]).type(dtype), requires_grad=False)\n",
    "\n",
    "# print(np.shape(x),x)\n",
    "\n",
    "# print(np.shape(y),y)\n",
    "\n",
    "w1_y = torch.FloatTensor(input_size, hidden_size).type(dtype)\n",
    "init.normal(w1_y, 0, 0.3)\n",
    "# w1_y=torch.tensor([[ 0.6040, -0.7184, -0.4734,  0.3292],\n",
    "#         [ 0.4197, -0.1070,  0.0947, -0.7358],\n",
    "#         [ 0.2507,  0.0215,  0.4250,  0.0370],\n",
    "#         [-0.2239,  0.4435, -0.1628,  0.5767],\n",
    "#         [ 0.3655,  0.0549, -0.1239,  0.2886]], requires_grad=True)\n",
    "w1_y =  Variable(w1_y, requires_grad=True)\n",
    "print(np.shape(w1_y))\n",
    "w2_y = torch.FloatTensor(hidden_size, output_size).type(dtype)\n",
    "init.normal(w2_y, 0, 0.4)\n",
    "# w2_y=torch.tensor([[ 0.9871],\n",
    "#         [-0.6229],\n",
    "#         [-0.4600],\n",
    "#         [ 0.2643]], requires_grad=True)\n",
    "\n",
    "w2_y = Variable(w2_y, requires_grad=True)\n",
    "print(np.shape(w2_y))\n",
    "def forward(input, context_state, w1_y, w2_y):\n",
    "  xh = torch.cat((input, context_state), 1)\n",
    "  context_state = torch.tanh(xh.mm(w1_y))\n",
    "  out = context_state.mm(w2_y)\n",
    "  return  (out, context_state)\n",
    "for i in range(epochs):\n",
    "  total_loss = 0\n",
    "  context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=True)\n",
    "  for j in range(y.size(0)):\n",
    "    input = rot_x[j:(j+1)]\n",
    "    target = y[j:(j+1)]\n",
    "    (pred, context_state) = forward(input, context_state, w1_y, w2_y)\n",
    "    loss = (pred - target).pow(2).sum()/2\n",
    "    total_loss += loss\n",
    "    loss.backward()\n",
    "    w1_y.data -= lr * w1_y.grad.data\n",
    "    w2_y.data -= lr * w2_y.grad.data\n",
    "    w1_y.grad.data.zero_()\n",
    "    w2_y.grad.data.zero_()\n",
    "    context_state = Variable(context_state.data)\n",
    "  if i % 20 == 0:\n",
    "     print(\"Epoch: {} loss {}\".format(i, total_loss.data))\n",
    "print(\"w1: \",w1_y)\n",
    "print(\"w2: \",w2_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction Y\n",
    "\n",
    "context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=False)\n",
    "predictions = []\n",
    "Y_predict=[]\n",
    "data_=Rot_X_predicted.copy(order='C')\n",
    "seq_length=len(data_)\n",
    "data_.resize((seq_length + 1, 1))\n",
    "temp_ = Variable(torch.Tensor(data_[:-1]).type(dtype), requires_grad=False)\n",
    "\n",
    "for i in range(temp_.size(0)):\n",
    "  input = temp_[i:i+1]\n",
    "  (pred, context_state) = forward(input, context_state, w1_y, w2_y)\n",
    "  context_state = context_state\n",
    "  predictions.append(pred.data.numpy().ravel()[0])\n",
    "\n",
    "Y_predict=np.divide(predictions,max(predictions))\n",
    "Y_predict=np.multiply(Y_predict,max_Y_train)\n",
    "# plt.plot(Y)\n",
    "# plt.plot(Y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_squared_train_y:  25576.16769397355 1917.4946513360949\n"
     ]
    }
   ],
   "source": [
    "#MSE error:\n",
    "\n",
    "#X,Y,Z error\n",
    "\n",
    "actual_data_y=Y[0:len(Y_predict)][-20*prediction_time:]\n",
    "\n",
    "predicted_data_y=Y_predict[-20*prediction_time:]\n",
    "\n",
    "#error for y\n",
    "ss_res_train=[]\n",
    "for k in range(len(predicted_data_y)):\n",
    "    ss_res_train.append((actual_data_y[k]- predicted_data_y[k])**2)\n",
    "ss_tot_train=[]\n",
    "for k in range(len(predicted_data_y)):\n",
    "    ss_tot_train.append((actual_data_y[k]-np.mean(actual_data_y))**2)\n",
    "r_squared_train_y = 1 - (np.sum( ss_res_train)/np.sum(ss_tot_train))\n",
    "\n",
    "print(\"r_squared_train_y: \",np.sum( ss_res_train),np.sum(ss_tot_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:  tensor([[-0.8976, -0.0443, -0.3203,  0.1730, -0.0658, -0.2087],\n",
      "        [ 0.0131, -0.1364, -0.0186,  0.2386,  0.0117, -0.3229],\n",
      "        [ 0.1395, -0.0851,  0.2515,  0.0590,  0.4586,  0.1279],\n",
      "        [ 0.5605, -0.1500, -0.2014, -0.1581,  0.1909, -0.0185],\n",
      "        [-0.1277, -0.3580, -0.0395, -0.2905, -0.0605,  0.0347],\n",
      "        [ 0.4607, -0.0569, -0.1329, -0.1055,  0.0483,  0.0342],\n",
      "        [-0.1916, -0.0462, -0.3716,  0.1053, -0.1005, -0.7972]],\n",
      "       requires_grad=True)\n",
      "w2:  tensor([[ 0.3457],\n",
      "        [-0.7237],\n",
      "        [ 0.1072],\n",
      "        [ 0.8113],\n",
      "        [-0.6045],\n",
      "        [-0.2506]], requires_grad=True)\n",
      "Epoch: 0 loss 4.5810699462890625\n",
      "Epoch: 100 loss 4.484375476837158\n",
      "Epoch: 200 loss 4.4062323570251465\n",
      "Epoch: 300 loss 4.342918395996094\n",
      "Epoch: 400 loss 4.291516304016113\n"
     ]
    }
   ],
   "source": [
    "#Frequency prediction\n",
    "#https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/\n",
    "#success with Rot_Y\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import torch.nn.init as init\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "input_size, hidden_size, output_size = 7, 6, 1\n",
    "epochs = 500\n",
    "# seq_length = 20\n",
    "lr = 0.00005 #0.00005\n",
    "\n",
    "# data_time_steps = np.linspace(2, 10, seq_length + 1)\n",
    "# data = np.sin(data_time_steps)\n",
    "\n",
    "y_=np.divide(fft_Rot_Y_short,max(fft_Rot_Y_short))\n",
    "max_y_fft_Rot_Y_short=max(fft_Rot_Y_short)\n",
    "\n",
    "data=y_.copy(order='C')\n",
    "seq_length=len(data)\n",
    "\n",
    "data.resize((seq_length + 1, 1))\n",
    "\n",
    "x = Variable(torch.Tensor(data[:-1]).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.Tensor(data[1:]).type(dtype), requires_grad=False)\n",
    "\n",
    "w1_Rot_Y = torch.FloatTensor(input_size, hidden_size).type(dtype)\n",
    "# init.normal(w1, 0, 0.3)\n",
    "w1_Rot_Y=torch.tensor([[-0.8976, -0.0443, -0.3203,  0.1730, -0.0658, -0.2087],\n",
    "        [ 0.0131, -0.1364, -0.0186,  0.2386,  0.0117, -0.3229],\n",
    "        [ 0.1395, -0.0851,  0.2515,  0.0590,  0.4586,  0.1279],\n",
    "        [ 0.5605, -0.1500, -0.2014, -0.1581,  0.1909, -0.0185],\n",
    "        [-0.1277, -0.3580, -0.0395, -0.2905, -0.0605,  0.0347],\n",
    "        [ 0.4607, -0.0569, -0.1329, -0.1055,  0.0483,  0.0342],\n",
    "        [-0.1916, -0.0462, -0.3716,  0.1053, -0.1005, -0.7972]],requires_grad=True)\n",
    "w1_Rot_Y =  Variable(w1_Rot_Y, requires_grad=True)\n",
    "print(\"w1: \",w1_Rot_Y)\n",
    "\n",
    "w2_Rot_Y = torch.FloatTensor(hidden_size, output_size).type(dtype)\n",
    "# init.normal(w2, 0, 0.4)\n",
    "w2_Rot_Y=torch.tensor([[ 0.3457],\n",
    "        [-0.7237],\n",
    "        [ 0.1072],\n",
    "        [ 0.8113],\n",
    "        [-0.6045],\n",
    "        [-0.2506]], requires_grad=True)\n",
    "w2_Rot_Y = Variable(w2_Rot_Y, requires_grad=True)\n",
    "print(\"w2: \",w2_Rot_Y)\n",
    "\n",
    "def forward(input, context_state, w1_Rot_Y, w2_Rot_Y):\n",
    "  xh = torch.cat((input, context_state), 1)\n",
    "  context_state = torch.tanh(xh.mm(w1_Rot_Y))\n",
    "  out = context_state.mm(w2_Rot_Y)\n",
    "  return  (out, context_state)\n",
    "\n",
    "for i in range(epochs):\n",
    "  total_loss = 0\n",
    "  context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=True)\n",
    "\n",
    "  for j in range(x.size(0)):\n",
    "    input = x[j:(j+1)]\n",
    "    target = y[j:(j+1)]\n",
    "    (pred, context_state) = forward(input, context_state, w1_Rot_Y, w2_Rot_Y)\n",
    "    loss = (pred - target).pow(2).sum()/2\n",
    "    total_loss += loss\n",
    "    loss.backward()\n",
    "    w1_Rot_Y.data -= lr * w1_Rot_Y.grad.data\n",
    "    w2_Rot_Y.data -= lr * w2_Rot_Y.grad.data\n",
    "    w1_Rot_Y.grad.data.zero_()\n",
    "    w2_Rot_Y.grad.data.zero_()\n",
    "    context_state = Variable(context_state.data)\n",
    "  if i % 100 == 0:\n",
    "     print(\"Epoch: {} loss {}\".format(i, total_loss.data))\n",
    "\n",
    "context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=False)\n",
    "predictions_Rot_Y_short = []\n",
    "\n",
    "for i in range(x.size(0)):\n",
    "  input = x[i:i+1]\n",
    "  (pred, context_state) = forward(input, context_state, w1_Rot_Y, w2_Rot_Y)\n",
    "  context_state = context_state\n",
    "  predictions_Rot_Y_short.append(pred.data.numpy().ravel()[0])\n",
    "    \n",
    "# plt.plot(y_)\n",
    "# plt.plot(predictions_Rot_Y_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predicted wave\n",
    "Rot_Y_predicted=ifft([0]*100+predictions_Rot_Y_short[100:-1] + [0]*19580+[0]*prediction_time*20)\n",
    "temp_fft_Rot_Y_train_reverse=np.divide( Rot_Y_predicted,max(Rot_Y_predicted))\n",
    "Rot_Y_predicted=np.multiply(temp_fft_Rot_Y_train_reverse, max_Rot_Y_train)\n",
    "# print((Rot_Y_predicted))\n",
    "# print(len(Rot_Y_predicted))\n",
    "# plt.plot(Rot_Y_predicted,c='r')\n",
    "# plt.plot(Rot_Y_train)\n",
    "# plt.plot(np.multiply(temp_y_fft_Rot_Y_train_reverse,max_y_fft_Rot_Y_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4])\n",
      "torch.Size([4, 1])\n",
      "Epoch: 0 loss 210794.734375\n",
      "w1:  tensor([[-0.1625, -2.1598, -1.9456, -1.1110],\n",
      "        [ 1.3065, -0.1157,  0.1000, -0.3820],\n",
      "        [ 1.0104,  0.2343,  0.6588,  0.5346],\n",
      "        [ 0.5178,  0.6113,  0.0239,  1.0314],\n",
      "        [ 1.0372, -0.0249, -0.1954,  0.5033]], requires_grad=True)\n",
      "w2:  tensor([[3.7694],\n",
      "        [2.4965],\n",
      "        [2.5038],\n",
      "        [2.7719]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/\n",
    "#success with Rot_Y vs z\n",
    "\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "input_size, hidden_size, output_size = 5, 4, 1\n",
    "epochs = 100\n",
    "# seq_length = 20\n",
    "lr = 0.00006 #0.00005\n",
    "\n",
    "# data_time_steps = np.linspace(2, 10, seq_length + 1)\n",
    "# data = np.sin(data_time_steps)\n",
    "\n",
    "Rot_Y_=np.divide(Rot_Y_train,max(Rot_Y_train))\n",
    "# print(y_fft_short)\n",
    "data_Rot_Y=Rot_Y_.copy(order='C')\n",
    "seq_length=len(data_Rot_Y)\n",
    "data_Rot_Y.resize((seq_length + 1, 1))\n",
    "Y_train_max=max(Y_train)\n",
    "\n",
    "Z_=np.divide(Z_train,max(Z_train))\n",
    "# print(y_fft_short)\n",
    "data_Z=Z_.copy(order='C')\n",
    "seq_length=len(data_Z)\n",
    "data_Z.resize((seq_length + 1, 1))\n",
    "\n",
    "# print(np.shape(data),data)\n",
    "\n",
    "z = Variable(torch.Tensor(data_Z[1:]).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.Tensor(data_Rot_Y[1:]).type(dtype), requires_grad=False)\n",
    "\n",
    "# print(np.shape(x),x)\n",
    "\n",
    "# print(np.shape(y),y)\n",
    "\n",
    "w1_z = torch.FloatTensor(input_size, hidden_size).type(dtype)\n",
    "# init.normal(w1, 0, 0.3)\n",
    "w1_z=torch.tensor([[ 0.6040, -0.7184, -0.4734,  0.3292],\n",
    "        [ 0.4197, -0.1070,  0.0947, -0.7358],\n",
    "        [ 0.2507,  0.0215,  0.4250,  0.0370],\n",
    "        [-0.2239,  0.4435, -0.1628,  0.5767],\n",
    "        [ 0.3655,  0.0549, -0.1239,  0.2886]], requires_grad=True)\n",
    "w1_z =  Variable(w1_z, requires_grad=True)\n",
    "print(np.shape(w1_z))\n",
    "w2_z = torch.FloatTensor(hidden_size, output_size).type(dtype)\n",
    "# init.normal(w2, 0, 0.4)\n",
    "w2_z=torch.tensor([[ 0.9871],\n",
    "        [-0.6229],\n",
    "        [-0.4600],\n",
    "        [ 0.2643]], requires_grad=True)\n",
    "\n",
    "w2_z = Variable(w2_z, requires_grad=True)\n",
    "print(np.shape(w2_z))\n",
    "def forward(input, context_state, w1, w2):\n",
    "  xh = torch.cat((input, context_state), 1)\n",
    "  context_state = torch.sigmoid(xh.mm(w1))\n",
    "  out = context_state.mm(w2)\n",
    "  return  (out, context_state)\n",
    "for i in range(epochs):\n",
    "  total_loss = 0\n",
    "  context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=True)\n",
    "  for j in range(z.size(0)):\n",
    "    input = y[j:(j+1)]\n",
    "    target = z[j:(j+1)]\n",
    "    (pred, context_state) = forward(input, context_state, w1_z, w2_z)\n",
    "    loss = (pred - target).pow(2).sum()/2\n",
    "    total_loss += loss\n",
    "    loss.backward()\n",
    "    w1_z.data -= lr * w1_z.grad.data\n",
    "    w2_z.data -= lr * w2_z.grad.data\n",
    "    w1_z.grad.data.zero_()\n",
    "    w2_z.grad.data.zero_()\n",
    "    context_state = Variable(context_state.data)\n",
    "  if i % 100 == 0:\n",
    "     print(\"Epoch: {} loss {}\".format(i, total_loss.data))\n",
    "print(\"w1: \",w1_z)\n",
    "print(\"w2: \",w2_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction z\n",
    "\n",
    "context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=False)\n",
    "predictions = []\n",
    "\n",
    "data_=Rot_Y_predicted.copy(order='C')\n",
    "seq_length=len(data_)\n",
    "data_.resize((seq_length + 1, 1))\n",
    "temp_ = Variable(torch.Tensor(data_[:-1]).type(dtype), requires_grad=False)\n",
    "\n",
    "for i in range(temp_.size(0)):\n",
    "  input = temp_[i:i+1]\n",
    "  (pred, context_state) = forward(input, context_state, w1_z, w2_z)\n",
    "  context_state = context_state\n",
    "  predictions.append(pred.data.numpy().ravel()[0])\n",
    "\n",
    "z_predict=np.divide(predictions,max(predictions))\n",
    "z_predict=np.multiply(z_predict,max_Z_train)\n",
    "# plt.plot(y_)\n",
    "# plt.plot(z_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19980\n",
      "20080\n",
      "20080\n",
      "19980 19980\n",
      "velocity (m/s):  15.398073542655387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# predict X\n",
    "# Insert Modeling Building or Plotting code here\n",
    "# Note, you may implement these however you see fit\n",
    "# Ex: using an existing library, solving the Normal Eqns\n",
    "#     implementing your own SGD solver for them. Your Choice.\n",
    "\n",
    "\n",
    "#Load data\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "X_=time_train.to_numpy()\n",
    "y_=X_train\n",
    "X_ = X_.reshape(-1,1)\n",
    "\n",
    "X_train_=X_\n",
    "y_train_=y_\n",
    "# X_test=time[:len(X_train)+prediction_time*20]\n",
    "X_test_=X_\n",
    "y_test_=y_\n",
    "\n",
    "print(len(X_train_))\n",
    "\n",
    "import pandas\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "def test_func1(x_, dist, b):\n",
    "    return dist + b* x_\n",
    "\n",
    "ratio=0.97752809\n",
    "plt.figure(figsize=(10,10))\n",
    "n=1\n",
    "r_squared_array=np.zeros(shape=(6,4))\n",
    "r_squared_array_train=np.zeros(shape=(6,4))\n",
    "\n",
    "# plt.subplot(6,4,n)\n",
    "y_=[]\n",
    "x=[]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#  X, y, test_size=ratio, random_state=0)   #0.101123595505618,0.97752808988764\n",
    "\n",
    "\n",
    "\n",
    "for i in X_train_:\n",
    "   x.append(i[0]) \n",
    "\n",
    "# plt.plot(x,y_train_)\n",
    "n=n+1\n",
    "fy_test=[]\n",
    "fy_train=[]\n",
    "params, params_covariance = curve_fit(test_func1, x, y_train_)\n",
    "for i in X_test_:\n",
    "    fy_test.append(test_func1(i,*params))\n",
    "for i in X_train_:\n",
    "    fy_train.append(test_func1(i,*params))\n",
    "\n",
    "text= 'Linear'\n",
    "ss_res=[]\n",
    "for k in range(len(fy_test)):\n",
    "    ss_res.append((y_test_[k]- fy_test[k])**2)\n",
    "ss_tot=[]\n",
    "for k in range(len(fy_test)):\n",
    "    ss_tot.append((y_test_[k]-np.mean(y_test_))**2)\n",
    "r_squared = 1 - (np.sum( ss_res)/np.sum(ss_tot))\n",
    "\n",
    "ss_res_train=[]\n",
    "for k in range(len(fy_train)):\n",
    "    ss_res_train.append((y_train_[k]- fy_train[k])**2)\n",
    "ss_tot_train=[]\n",
    "for k in range(len(fy_train)):\n",
    "    ss_tot_train.append((y_train_[k]-np.mean(y_train_))**2)\n",
    "r_squared_train = 1 - (np.sum( ss_res_train)/np.sum(ss_tot_train))\n",
    "r_squared_array=r_squared\n",
    "r_squared_array_train=r_squared_train\n",
    "\n",
    "temp=[i[0] for i in ss_res]\n",
    "X_test_=[i[0] for i in X_test_]\n",
    "\n",
    "print(len(X_)+prediction_time*20)\n",
    "\n",
    "x_test_= time.to_numpy()[:len(time_train)+prediction_time*20]\n",
    "print(len(x_test_))\n",
    "Y_test_= y_\n",
    "\n",
    "print(len(X_train_),len(fy_train))\n",
    "\n",
    "X_predict=params[0]+params[1]*x_test_\n",
    "\n",
    "# plt.scatter(x_test_, X_predict,s=10,c='Black')\n",
    "# plt.scatter(X_train_, fy_train,s=10,c='Red')\n",
    "\n",
    "# plt.title(\" Type: \"+text+\"\\nTrain score = {:.3f}\\n N of test size = {:.0f}\".format( r_squared,2225-ratio*2225))\n",
    "# print(params)\n",
    "velocity=params[1]*0.3048\n",
    "print(\"velocity (m/s): \",velocity)\n",
    "# plt.scatter([1,2,3,4], r_squared_array,s=10,c='Black')\n",
    "# plt.title(\" Type: \"+text+\"\\nTrain score = {:.3f}\\n N of test size = {:.0f}\".format( r_squared,2225-ratio[j]*2225))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1:  tensor([[-0.0772,  0.6960, -0.4124,  0.0416, -0.2429],\n",
      "        [-0.0684, -0.3990,  0.5290,  0.0226,  0.2052],\n",
      "        [ 0.2327, -0.5188,  0.0701, -0.3986, -0.2465],\n",
      "        [-0.2093,  0.4494,  0.3747,  0.1062,  0.2342],\n",
      "        [-0.2192, -0.3744,  0.0869, -0.1836, -0.4926],\n",
      "        [ 0.1669, -0.5710,  0.1060, -0.2503, -0.0245]], requires_grad=True)\n",
      "w2:  tensor([[ 0.2551],\n",
      "        [-0.4843],\n",
      "        [-0.0811],\n",
      "        [ 0.0319],\n",
      "        [-0.1918]], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ebcd758ffe3e>:32: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(w1_Rot_Z, 0, 0.3)\n",
      "<ipython-input-14-ebcd758ffe3e>:45: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(w2_Rot_Z, 0, 0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 loss 14.192745208740234\n",
      "Epoch: 100 loss 3.489047050476074\n",
      "Epoch: 200 loss 2.001464605331421\n",
      "Epoch: 300 loss 1.489052176475525\n",
      "Epoch: 400 loss 1.226842999458313\n"
     ]
    }
   ],
   "source": [
    "#Frequency prediction\n",
    "#https://www.cpuheater.com/deep-learning/introduction-to-recurrent-neural-networks-in-pytorch/\n",
    "#success with Rot_Z\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "import torch.nn.init as init\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "input_size, hidden_size, output_size = 6, 5, 1\n",
    "epochs = 500\n",
    "# seq_length = 20\n",
    "lr = 0.00005 #0.00005\n",
    "\n",
    "# data_time_steps = np.linspace(2, 10, seq_length + 1)\n",
    "# data = np.sin(data_time_steps)\n",
    "\n",
    "y_=np.divide(fft_Rot_Z_short,max(fft_Rot_Z_short))\n",
    "max_fft_Rot_Z_short=max(fft_Rot_Z_short)\n",
    "\n",
    "data=y_.copy(order='C')\n",
    "seq_length=len(data)\n",
    "\n",
    "data.resize((seq_length + 1, 1))\n",
    "\n",
    "x = Variable(torch.Tensor(data[:-1]).type(dtype), requires_grad=False)\n",
    "y = Variable(torch.Tensor(data[1:]).type(dtype), requires_grad=False)\n",
    "\n",
    "w1_Rot_Z = torch.FloatTensor(input_size, hidden_size).type(dtype)\n",
    "init.normal(w1_Rot_Z, 0, 0.3)\n",
    "# w1_Rot_Z=torch.tensor([[-0.8976, -0.0443, -0.3203,  0.1730, -0.0658, -0.2087],\n",
    "#         [ 0.0131, -0.1364, -0.0186,  0.2386,  0.0117, -0.3229],\n",
    "#         [ 0.1395, -0.0851,  0.2515,  0.0590,  0.4586,  0.1279],\n",
    "#         [ 0.5605, -0.1500, -0.2014, -0.1581,  0.1909, -0.0185],\n",
    "#         [-0.1277, -0.3580, -0.0395, -0.2905, -0.0605,  0.0347],\n",
    "#         [ 0.4607, -0.0569, -0.1329, -0.1055,  0.0483,  0.0342],\n",
    "#         [-0.1916, -0.0462, -0.3716,  0.1053, -0.1005, -0.7972]],\n",
    "#        requires_grad=True)\n",
    "w1_Rot_Z =  Variable(w1_Rot_Z, requires_grad=True)\n",
    "print(\"w1: \",w1_Rot_Z)\n",
    "\n",
    "w2_Rot_Z = torch.FloatTensor(hidden_size, output_size).type(dtype)\n",
    "init.normal(w2_Rot_Z, 0, 0.4)\n",
    "# w2_Rot_Z=torch.tensor([[ 0.3457],\n",
    "#         [-0.7237],\n",
    "#         [ 0.1072],\n",
    "#         [ 0.8113],\n",
    "#         [-0.6045],\n",
    "#         [-0.2506]], requires_grad=True)\n",
    "w2_Rot_Z = Variable(w2_Rot_Z, requires_grad=True)\n",
    "print(\"w2: \",w2_Rot_Z)\n",
    "\n",
    "def forward(input, context_state, w1_Rot_Z, w2_Rot_Z):\n",
    "  xh = torch.cat((input, context_state), 1)\n",
    "  context_state = torch.tanh(xh.mm(w1_Rot_Z))\n",
    "  out = context_state.mm(w2_Rot_Z)\n",
    "  return  (out, context_state)\n",
    "\n",
    "for i in range(epochs):\n",
    "  total_loss = 0\n",
    "  context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=True)\n",
    "\n",
    "  for j in range(x.size(0)):\n",
    "    input = x[j:(j+1)]\n",
    "    target = y[j:(j+1)]\n",
    "    (pred, context_state) = forward(input, context_state, w1_Rot_Z, w2_Rot_Z)\n",
    "    loss = (pred - target).pow(2).sum()/2\n",
    "    total_loss += loss\n",
    "    loss.backward()\n",
    "    w1_Rot_Z.data -= lr * w1_Rot_Z.grad.data\n",
    "    w2_Rot_Z.data -= lr * w2_Rot_Z.grad.data\n",
    "    w1_Rot_Z.grad.data.zero_()\n",
    "    w2_Rot_Z.grad.data.zero_()\n",
    "    context_state = Variable(context_state.data)\n",
    "  if i % 100 == 0:\n",
    "     print(\"Epoch: {} loss {}\".format(i, total_loss.data))\n",
    "\n",
    "context_state = Variable(torch.zeros((1, hidden_size)).type(dtype), requires_grad=False)\n",
    "predictions_Rot_Z_short = []\n",
    "\n",
    "for i in range(x.size(0)):\n",
    "  input = x[i:i+1]\n",
    "  (pred, context_state) = forward(input, context_state, w1_Rot_Z, w2_Rot_Z)\n",
    "  context_state = context_state\n",
    "  predictions_Rot_Z_short.append(pred.data.numpy().ravel()[0])\n",
    "    \n",
    "# plt.plot(y_)\n",
    "# plt.plot(predictions_Rot_Z_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3753326\n"
     ]
    }
   ],
   "source": [
    "#generate predicted Rot_Z wave\n",
    "Rot_Z_predicted=ifft(predictions_Rot_Z_short[0:-1] + [0]*19580+[0]*prediction_time*20)\n",
    "temp_fft_Rot_Z_train_reverse=np.divide( Rot_Z_predicted,max(Rot_Z_predicted))\n",
    "Rot_Z_predicted=np.multiply(temp_fft_Rot_Z_train_reverse, max_Rot_Z_train)\n",
    "print((max_Rot_Z_train))\n",
    "# print(len(Rot_Y_predicted))\n",
    "# plt.plot((Rot_Z_predicted),c='r')\n",
    "# plt.plot(Rot_Z_train)\n",
    "# plt.plot(np.multiply(temp_y_fft_Rot_Y_train_reverse,max_y_fft_Rot_Y_short))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Rot_Z:  0.06003781993471812\n"
     ]
    }
   ],
   "source": [
    "#Rot_X,Y,Z error\n",
    "\n",
    "actual_data_Rot_Z=Rot_Z[0:len(Rot_Z_predicted)][-20*prediction_time:]\n",
    "\n",
    "predicted_data_Rot_Z=Rot_Z_predicted[-20*prediction_time:]\n",
    "\n",
    "#error for z\n",
    "ss_res_train_Rot_Z=[]\n",
    "for k in range(len(predicted_data_Rot_Z)):\n",
    "    ss_res_train_Rot_Z.append(np.abs((actual_data_Rot_Z[k]- predicted_data_Rot_Z[k])**2))\n",
    "\n",
    "\n",
    "print(\"MSE_Rot_Z: \",np.power(sum(ss_res_train_Rot_Z),0.5)/len(ss_res_train_Rot_Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_x:  1.8514792469534336\n",
      "MSE_y:  1.5992550670225671\n",
      "MSE_z:  1.6088021417915113\n"
     ]
    }
   ],
   "source": [
    "#MSE error:\n",
    "\n",
    "#X,Y,Z error\n",
    "\n",
    "actual_data_x=X[0:len(X_predict)][-20*prediction_time:]\n",
    "actual_data_y=Y[0:len(Y_predict)][-20*prediction_time:]\n",
    "actual_data_z=Z[0:len(z_predict)][-20*prediction_time:]\n",
    "\n",
    "predicted_data_x=X_predict[-20*prediction_time:]\n",
    "predicted_data_y=Y_predict[-20*prediction_time:]\n",
    "predicted_data_z=z_predict[-20*prediction_time:]\n",
    "\n",
    "# plt.plot(predicted_data_x,actual_data_x)\n",
    "\n",
    "#error for x\n",
    "ss_res_train_x=[]\n",
    "for k in range(len(predicted_data_x)):\n",
    "    ss_res_train_x.append((actual_data_x[k]- predicted_data_x[k])**2)\n",
    "\n",
    "#error for y\n",
    "ss_res_train_y=[]\n",
    "for k in range(len(predicted_data_y)):\n",
    "    ss_res_train_y.append((actual_data_y[k]- predicted_data_y[k])**2)\n",
    "\n",
    "#error for z\n",
    "ss_res_train_z=[]\n",
    "for k in range(len(predicted_data_z)):\n",
    "    ss_res_train_z.append((actual_data_z[k]- predicted_data_z[k])**2)\n",
    "\n",
    "print(\"MSE_x: \",np.power(sum(ss_res_train_x),0.5)/len(predicted_data_x))\n",
    "print(\"MSE_y: \",np.power(sum(ss_res_train_y),0.5)/len(predicted_data_y))\n",
    "print(\"MSE_z: \",np.power(sum(ss_res_train_z),0.5)/len(predicted_data_z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE_Rot_X:  0.3260607043373058\n",
      "MSE_Rot_Y:  0.22075573439354623\n",
      "MSE_Rot_Z:  0.06003781993471812\n",
      "Time to move 1 meter:  0.06494318897944104\n"
     ]
    }
   ],
   "source": [
    "#Rot_X,Y,Z error\n",
    "\n",
    "actual_data_Rot_X=Rot_X[0:len(Rot_X_predicted)][-20*prediction_time:]\n",
    "actual_data_Rot_Y=Rot_Y[0:len(Rot_Y_predicted)][-20*prediction_time:]\n",
    "actual_data_Rot_Z=Rot_Z[0:len(Rot_Z_predicted)][-20*prediction_time:]\n",
    "\n",
    "predicted_data_Rot_X=Rot_X_predicted[-20*prediction_time:]\n",
    "predicted_data_Rot_Y=Rot_Y_predicted[-20*prediction_time:]\n",
    "predicted_data_Rot_Z=Rot_Z_predicted[-20*prediction_time:]\n",
    "\n",
    "\n",
    "#error for x\n",
    "ss_res_train_Rot_X=[]\n",
    "for k in range(len(predicted_data_Rot_X)):\n",
    "    ss_res_train_Rot_X.append(np.abs((actual_data_Rot_X[k]- predicted_data_Rot_X[k])**2))\n",
    "\n",
    "#error for y\n",
    "ss_res_train_Rot_Y=[]\n",
    "for k in range(len(predicted_data_Rot_Y)):\n",
    "    ss_res_train_Rot_Y.append(np.abs((actual_data_Rot_Y[k]- predicted_data_Rot_Y[k])**2))\n",
    "\n",
    "#error for z\n",
    "ss_res_train_Rot_Z=[]\n",
    "for k in range(len(predicted_data_Rot_Z)):\n",
    "    ss_res_train_Rot_Z.append(np.abs((actual_data_Rot_Z[k]- predicted_data_Rot_Z[k])**2))\n",
    "\n",
    "\n",
    "\n",
    "print(\"MSE_Rot_X: \",np.power(sum(ss_res_train_Rot_X),0.5)/len(ss_res_train_Rot_X))\n",
    "print(\"MSE_Rot_Y: \",np.power(sum(ss_res_train_Rot_Y),0.5)/len(ss_res_train_Rot_Y))\n",
    "print(\"MSE_Rot_Z: \",np.power(sum(ss_res_train_Rot_Z),0.5)/len(ss_res_train_Rot_Z))\n",
    "\n",
    "print(\"Time to move 1 meter: \",1/velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
